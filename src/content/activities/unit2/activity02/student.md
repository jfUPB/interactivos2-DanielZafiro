Al aplicar el método INPUT/OUTPUT de Patrik Hübner a estos tres eventos, vemos cómo inputs heterogéneos (chat, sensores, código) se transforman mediante procesos algorítmicos en outputs experienciales (juego modificado, paisajes de luz, performance audiovisual) y, a través del **storytelling**, generan narrativas colectivas e inmersivas. Esto ilustra el potencial del diseño generativo interactivo para convertir espectadores pasivos en coautores de la experiencia en tiempo real.

---

#### Evento 1: Transmisiones Interactivas en Twitch

**Momento seleccionado:** La votación colectiva en el canal de DunkOrSlam para influir en los poderes del streamer en *Noita*.

1. **Inputs**  
   - Mensajes de chat (comandos específicos) y Bits utilizados por los espectadores.  
   - Datos de suscripción, cheer y otras métricas de interacción que activan eventos.  

2. **Procesamiento**  
   - Una extensión (p. ej. Crowd Control) lee el chat y traduce Bits/comandos en triggers de juego.  
   - Lógica de agregación de votos: cada 30 s se cuentan las opciones más votadas.  
   - Algoritmos de mapeo que convierten el recuento en acciones del juego (p. ej. “activar trueno” o “dar escudo”). citeturn0search0

3. **Outputs**  
   - Cambios en tiempo real dentro del juego (*Noita*): aparición de efectos visuales, modificación de física, alteración del entorno.  
   - Overlays en pantalla que muestran resultados de la votación y estado de la interacción.

4. **Storytelling**  
   - Surge una narrativa **emergente**: el futuro del stream se escribe colectivamente.  
   - Cada fase de votación añade un “capítulo” inesperado al recorrido del streamer, generando un relato único en cada sesión.  

---

#### Evento 2: teamLab Planets Tokyo

**Momento seleccionado:** El pasillo de luces LED que cambian de color al ritmo de tus pasos.

1. **Inputs**  
   - Señales de sensores de movimiento y presión en el suelo (video tracking).  
   - Datos de proximidad y velocidad de los visitantes.  

2. **Procesamiento**  
   - Un sistema de visión computacional interpreta posiciones y velocidades.  
   - Mapeo de parámetros de movimiento a valores HSB para los LEDs (hue, brillo, saturación). citeturn0search1  
   - Lógica de “campo reactivo”: un visitante genera un gradiente de color que se propaga en la instalación.

3. **Outputs**  
   - Secuencias de luz que fluyen y cambian en los muros y el suelo, respondiendo al desplazamiento de las personas.  
   - En algunos espacios, proyección de formas y reflejos en el agua que evolucionan según la interacción.

4. **Storytelling**  
   - La instalación se convierte en un **paisaje narrativo** donde cada visitante es protagonista.  
   - La “historia” se escribe con trazos de luz: pasos, agrupaciones y ritmos colectivos definen la experiencia sensorial.  

---

#### Evento 3: Flok WeekEndJam 20250329 (mot4i x savamala)

**Momento seleccionado:** El clímax sonoro donde mot4i introduce un nuevo patrón de percusión generado en código.

1. **Inputs**  
   - Código escrito en vivo por mot4i y savamala (parámetros rítmicos, notas, formas visuales).  
   - Sugerencias del chat y cambios de parámetros vía la interfaz de Flok.  

2. **Procesamiento**  
   - El entorno Flok ejecuta el código de Strudel (audio) y Hydra (visuales) en tiempo real.  
   - Funciones de generación algorítmica que toman las variables del código (tempo, semilla aleatoria) y producen patrones. citeturn0search0  
   - Filtrado y mezcla: líneas de código se combinan para modular la textura sonora y las capas gráficas.

3. **Outputs**  
   - Flujo de audio electrónico emergente con texturas cambiantes y evolutivas.  
   - Visuales en pantalla (formas geométricas "sensibles" al sonido) sincronizados con el sonido.  

4. **Storytelling**  
   - El set se presenta como un **diálogo codificado**: cada bloque de código narra un “verso” musical único en vivo.  
   - La evolución del código refleja una historia de exploración sonora, donde el artista y el público co-crean el viaje.  

