Al aplicar el método INPUT/OUTPUT de Patrik Hübner a estos tres eventos, vemos cómo inputs heterogéneos (chat, sensores, código) se transforman mediante procesos algorítmicos en outputs experienciales (juego modificado, paisajes de luz, performance audiovisual) y, a través del **storytelling**, generan narrativas colectivas e inmersivas. Esto ilustra el potencial del diseño generativo interactivo para convertir espectadores pasivos en coautores de la experiencia en tiempo real.

---

#### Evento 1: Transmisiones Interactivas en Twitch

**Momento seleccionado:** La votación colectiva en el canal de DunkOrSlam para influir en los poderes del streamer en *Noita*.

1. **Inputs**  
   - Mensajes de chat (comandos específicos) y Bits utilizados por los espectadores.  
   - Datos de suscripción, cheer y otras métricas de interacción que activan eventos.  

2. **Procesamiento**  
   - Una extensión (p. ej. Crowd Control) lee el chat y traduce Bits/comandos en triggers de juego.  
   - Lógica de agregación de votos: cada 30 s se cuentan las opciones más votadas.  
   - Algoritmos de mapeo que convierten el recuento en acciones del juego (p. ej. “activar trueno” o “dar escudo”).

3. **Outputs**  
   - Cambios en tiempo real dentro del juego (*Noita*): aparición de efectos visuales, modificación de física, alteración del entorno.  
   - Overlays en pantalla que muestran resultados de la votación y estado de la interacción.

4. **Storytelling y Conexión narrativa**  
- Surge una narrativa **emergente**: el futuro del stream se escribe colectivamente.  

   - **Storytelling:** Cada ronda de votos funciona como un “capítulo” que los espectadores escriben juntos, generando una historia emergente y única.

   - **Conexión narrativa:** Inputs (votos) → Procesamiento (agregación y mapeo) → Outputs (acciones en el juego) se enlazan en una secuencia coherente: el público se convierte en autor colectivo de la trama del stream, ofreciendo un relato interactivo en directo. Esto refleja cómo un buen storytelling inserta al usuario en el flujo narrativo, transformándolo de espectador pasivo a protagonista activo, y al streamer en un explorador de nuevas alternativas narrativas dentro del juego.

---

#### Evento 2: teamLab Planets Tokyo

**Momento seleccionado:** El pasillo de luces LED que cambian de color al ritmo de tus pasos.

1. **Inputs**  
   - Señales de sensores de movimiento y presión en el suelo (video tracking).  
   - Datos de proximidad y velocidad de los visitantes.  

2. **Procesamiento**  
   - Un sistema de visión computacional interpreta posiciones y velocidades.  
   - Mapeo de parámetros de movimiento a valores HSB para los LEDs (hue, brillo, saturación). 
   - Lógica de “campo reactivo”: un visitante genera un gradiente de color que se propaga en la instalación.

3. **Outputs**  
   - Secuencias de luz que fluyen y cambian en los muros y el suelo, respondiendo al desplazamiento de las personas.  
   - En algunos espacios, proyección de formas y reflejos en el agua que evolucionan según la interacción.

4. **Storytelling y Conexión narrativa**  
- La instalación se convierte en un **paisaje narrativo** donde cada visitante es protagonista.  

   - **Storytelling:** El espacio se narra como un ecosistema vivo, donde cada paso escribe un rastro de color.

   - **Conexión narrativa:** Inputs (movimiento) → Procesamiento (mapeo de color) → Outputs (luces reactivas) construyen un relato sensorial continuo: los visitantes co-escriben la trama lumínica del espacio, haciendo de su propio recorrido el hilo conductor de la narrativa

---

#### Evento 3: Flok WeekEndJam 20250329 (mot4i x savamala)

**Momento seleccionado:** El clímax sonoro donde mot4i introduce un nuevo patrón de percusión generado en código.

1. **Inputs**  
   - Código escrito en vivo por mot4i y savamala (parámetros rítmicos, notas, formas visuales).  
   - Sugerencias del chat y cambios de parámetros vía la interfaz de Flok.  

2. **Procesamiento**  
   - El entorno Flok ejecuta el código de Strudel (audio) y Hydra (visuales) en tiempo real.  
   - Funciones de generación algorítmica que toman las variables del código (tempo, semilla aleatoria) y producen patrones. 
   - Filtrado y mezcla: líneas de código se combinan para modular la textura sonora y las capas gráficas.

3. **Outputs**  
   - Flujo de audio electrónico emergente con texturas cambiantes y evolutivas.  
   - Visuales en pantalla (formas geométricas "sensibles" al sonido) sincronizados con el sonido.  

4. **Storytelling y Conexión narrativa**  
- El set se presenta como un **diálogo codificado**: cada bloque de código narra un “verso” musical único en vivo.  
   
   - **Storytelling:** Cada bloque de código actúa como un “verso” de una composición incesante, contando la evolución sonora.
 
   - **Conexión narrativa:** Inputs (código y chat) → Procesamiento (ejecución algorítmica) → Outputs (audio/visual) se enlazan en un relato codificado: los artistas y la audiencia co-crean un viaje musical, donde cada modificación de código avanza la historia sonora

